{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/hackaubi/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM \n",
    "from keras.utils import plot_model, multi_gpu_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "DATA_PATH = ''\n",
    "EMBEDDING_PATH = './wiki.en.vec'\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "512it [00:00, 5118.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519428it [04:10, 10053.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2519396 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open(EMBEDDING_PATH, encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key =  ['content', 'language', 'vote_up_count', 'vote_funny_count', 'comment_count']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "\n",
    "#输入文件路径，返回list;list内包含n个字典形式数据集\n",
    "def read_xlsx(path):\n",
    "    book = xlrd.open_workbook(path)\n",
    "    first_sheet = book.sheet_by_index(0)\n",
    "    result = []\n",
    "    index = 0\n",
    "    #print(first_sheet.row_values(3))\n",
    "    key = first_sheet.row_values(index)\n",
    "    print('key = ',key)\n",
    "    index += 1\n",
    "    while(1):\n",
    "        try:\n",
    "            #print(type(first_sheet.row_values(index)))\n",
    "            dic = dict(zip(key,first_sheet.row_values(index)))\n",
    "            result.append(dic)\n",
    "            index += 1\n",
    "        except Exception:\n",
    "            break\n",
    "    print('done')\n",
    "    # print('result = ',result)\n",
    "    return result, key\n",
    "\n",
    "xlsx_file = './reviews_language_helpful_funny_comment_stripped.xlsx'\n",
    "data_dict, label_name = read_xlsx(xlsx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zt(data):\n",
    "\tweight_vote_up = 1.0\n",
    "\tweight_vote_funny = 0.5\n",
    "\tweight_comment = 2.0\n",
    "\tmax = 0.0\n",
    "\ttemp = 0.0\n",
    "\tnew_data = list()\n",
    "\tfor votes in data:\n",
    "\t\tif(votes['vote_up_count']==0.0 and votes['vote_funny_count']==0.0 and votes['comment_count']==0.0):\n",
    "\t\t\tcontinue\n",
    "\t\tvotes['weight_temp']=votes['vote_up_count']*weight_vote_up + votes['vote_funny_count']*weight_vote_funny + votes[ 'comment_count']*weight_comment\n",
    "\t\tnew_data.append(votes)\n",
    "\t\tif(max < votes['weight_temp']):\n",
    "\t\t\tmax = votes['weight_temp']\n",
    "\n",
    "\tzt_list = list()\n",
    "\tzt_gay_list = list()\n",
    "\tfor i in range(int(max*2)+1):\n",
    "\t\tzt_list.append(0)\n",
    "\tfor votes in new_data:\n",
    "\t\tzt_list[int(votes['weight_temp']*2)]=zt_list[int(votes['weight_temp']*2)] + 1\n",
    "\tlenge = len(new_data)\n",
    "\tfor i in range(int(max*2)+1):\n",
    "\t\tzt_gay_list.append((temp+zt_list[i]/2)/lenge)\n",
    "\t\ttemp = temp + zt_list[i]\n",
    "\n",
    "\tfor votes in new_data:\n",
    "\t\tvotes['score'] = zt_gay_list[int(votes['weight_temp']*2)]\n",
    "\t\tif(votes['score']<0.2):\n",
    "\t\t\tvotes['class'] = [1,0,0,0]\n",
    "\t\telif(votes['score']<0.5):\n",
    "\t\t\tvotes['class'] = [0,1,0,0]\n",
    "\t\telif(votes['score']<0.8):\n",
    "\t\t\tvotes['class'] = [0,0,1,0]\n",
    "\t\telse:\n",
    "\t\t\tvotes['class'] = [0,0,0,1]\n",
    "\t\tdel votes['vote_up_count']\n",
    "\t\tdel votes['vote_funny_count']\n",
    "\t\tdel votes['comment_count']\n",
    "\t\tdel votes['weight_temp']\n",
    "\n",
    "\treturn new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'french', 'tchinese', 'czech', 'english', 'german', 'bulgarian', 'japanese', 'swedish', 'spanish', 'finnish', 'schinese', 'italian', 'russian', 'dutch', 'ukrainian', 'hungarian', 'brazilian', 'thai', 'greek', 'polish', 'danish', 'romanian', 'portuguese', 'norwegian', 'koreana', 'turkish'}\n"
     ]
    }
   ],
   "source": [
    "# languages\n",
    "languages = set()\n",
    "for data in data_dict:\n",
    "    languages.add(data['language'])\n",
    "print(languages)\n",
    "current_language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "current_language_data = [data for data in data_dict if data['language'] == current_language ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Wayyyyy better than GTA:IV',\n",
       "  'language': 'english',\n",
       "  'vote_up_count': 0.0,\n",
       "  'vote_funny_count': 0.0,\n",
       "  'comment_count': 1.0},\n",
       " {'content': 'IMO, the best GTA. If not just for the voice acting.',\n",
       "  'language': 'english',\n",
       "  'vote_up_count': 0.0,\n",
       "  'vote_funny_count': 0.0,\n",
       "  'comment_count': 1.0},\n",
       " {'content': 'Just awesome. Awesomeness by awesome.',\n",
       "  'language': 'english',\n",
       "  'vote_up_count': 1.0,\n",
       "  'vote_funny_count': 0.0,\n",
       "  'comment_count': 0.0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_language_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_language_data = zt(current_language_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs_train = [ data['content'] for data in current_language_data]\n",
    "labels = [ data['class'] for data in current_language_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(labels[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "#visualize word distribution\n",
    "# train_df['doc_len'] = train_df['comment_text'].apply(lambda words: len(words.split(\" \")))\n",
    "# max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "# sns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc len')\n",
    "# plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n",
    "# plt.title('comment length'); plt.legend()\n",
    "# plt.show()\n",
    "max_seq_len = 0\n",
    "for data in current_language_data:\n",
    "    max_seq_len = max(len(data['content']), max_seq_len)\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2618/24691 [00:00<00:01, 13081.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24691/24691 [00:00<00:00, 25051.35it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"pre-processing train data...\")\n",
    "processed_docs = []\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs.append(\" \".join(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wayyyyy better GTA IV',\n",
       " 'IMO best GTA If voice acting',\n",
       " 'Just awesome Awesomeness awesome',\n",
       " 'Gimme ticket']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  35996\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs)  #leaky\n",
    "word_seq = tokenizer.texts_to_sequences(processed_docs)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq = sequence.pad_sequences(word_seq, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUS:8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return len([x.name for x in local_device_protos if x.device_type == 'GPU'])\n",
    "num_gpus = get_available_gpus()\n",
    "print(\"GPUS:{}\".format(num_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 20 \n",
    "num_gpus = num_gpus\n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n",
    "\n",
    "#output parameters\n",
    "num_classes = 4\n",
    "\n",
    "#data split\n",
    "split_persentage = 0.8\n",
    "split_index = int(len(word_seq) * split_persentage)\n",
    "word_seq_train = word_seq[:split_index]\n",
    "word_seq_test = word_seq[split_index:]\n",
    "y_train = y_all[:split_index]\n",
    "y_test = y_all[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 9543\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample words not found:  ['credz' '33mbs' 'dnlfzsdlgijz' 'crawning' 'ตกลงๆ' 'skrr' '1776' 'entinen'\n",
      " 'rm150' 'recomendo' 'continuability' 'absurdamente' '150ish' 'fr8' '2d'\n",
      " 'ассасинов' 'ㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤ' 'iusse' 'الاخر' '안정화를']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample words not found: \", np.random.choice(words_not_found, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **BUG FIX**\n",
    "# BUG FIXED: cannot save model while using multi GPU \n",
    "\n",
    "from keras.layers import Lambda, concatenate\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def multi_gpu_model(model, gpus):\n",
    "    if isinstance(gpus, (list, tuple)):\n",
    "        num_gpus = len(gpus)\n",
    "        target_gpu_ids = gpus\n",
    "    else:\n",
    "        num_gpus = gpus\n",
    "        target_gpu_ids = range(num_gpus)\n",
    "\n",
    "    def get_slice(data, i, parts):\n",
    "        shape = tf.shape(data)\n",
    "        batch_size = shape[:1]\n",
    "        input_shape = shape[1:]\n",
    "        step = batch_size // parts\n",
    "        if i == num_gpus - 1:\n",
    "            size = batch_size - step * i\n",
    "        else:\n",
    "            size = step\n",
    "        size = tf.concat([size, input_shape], axis=0)\n",
    "        stride = tf.concat([step, input_shape * 0], axis=0)\n",
    "        start = stride * i\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    all_outputs = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        all_outputs.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU,\n",
    "    # each getting a slice of the inputs.\n",
    "    for i, gpu_id in enumerate(target_gpu_ids):\n",
    "        with tf.device('/gpu:%d' % gpu_id):\n",
    "            with tf.name_scope('replica_%d' % gpu_id):\n",
    "                inputs = []\n",
    "                # Retrieve a slice of the input.\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_i = Lambda(get_slice,\n",
    "                                     output_shape=input_shape,\n",
    "                                     arguments={'i': i,\n",
    "                                                'parts': num_gpus})(x)\n",
    "                    inputs.append(slice_i)\n",
    "\n",
    "                # Apply model on slice\n",
    "                # (creating a model replica on the target device).\n",
    "                outputs = model(inputs)\n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "\n",
    "                # Save the outputs for merging back together later.\n",
    "                for o in range(len(outputs)):\n",
    "                    all_outputs[o].append(outputs[o])\n",
    "\n",
    "    # Merge outputs on CPU.\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for name, outputs in zip(model.output_names, all_outputs):\n",
    "            merged.append(concatenate(outputs,\n",
    "                                      axis=0, name=name))\n",
    "        return Model(model.inputs, merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "Training using single GPU or CPU..\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 8000, 300)         10798800  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8000, 64)          134464    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 4000, 64)          28736     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               328704    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 11,299,060\n",
      "Trainable params: 500,260\n",
      "Non-trainable params: 10,798,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN LSTM architecture\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim,\n",
    "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "# model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "try:\n",
    "    model = ulti_gpu_model(model, gpus=num_gpus)\n",
    "    print(\"Training using multiple GPUs..\")\n",
    "except:\n",
    "    print(\"Training using single GPU or CPU..\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "# callbacks_list = [early_stopping]\n",
    "filepath = current_language + '.' + 'weights.ep{epoch:03d}.loss{loss:.3f}.val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17776 samples, validate on 1976 samples\n",
      "Epoch 1/20\n",
      "17776/17776 [==============================] - 338s 19ms/step - loss: 1.2938 - acc: 0.3881 - val_loss: 1.2665 - val_acc: 0.4671\n",
      "Epoch 2/20\n",
      "17776/17776 [==============================] - 327s 18ms/step - loss: 1.2624 - acc: 0.4080 - val_loss: 1.2385 - val_acc: 0.4970\n",
      "Epoch 3/20\n",
      "17776/17776 [==============================] - 328s 18ms/step - loss: 1.2483 - acc: 0.4203 - val_loss: 1.2504 - val_acc: 0.4722\n",
      "Epoch 4/20\n",
      "17776/17776 [==============================] - 329s 18ms/step - loss: 1.2262 - acc: 0.4384 - val_loss: 1.2219 - val_acc: 0.4919\n",
      "Epoch 5/20\n",
      "17776/17776 [==============================] - 329s 19ms/step - loss: 1.1939 - acc: 0.4570 - val_loss: 1.2451 - val_acc: 0.4626\n",
      "Epoch 6/20\n",
      "17776/17776 [==============================] - 325s 18ms/step - loss: 1.1278 - acc: 0.4918 - val_loss: 1.2823 - val_acc: 0.4706\n",
      "Epoch 7/20\n",
      "17776/17776 [==============================] - 327s 18ms/step - loss: 1.0407 - acc: 0.5277 - val_loss: 1.4503 - val_acc: 0.4762\n",
      "Epoch 8/20\n",
      "17776/17776 [==============================] - 326s 18ms/step - loss: 0.9430 - acc: 0.5763 - val_loss: 1.5287 - val_acc: 0.4453\n",
      "Epoch 9/20\n",
      "17776/17776 [==============================] - 328s 18ms/step - loss: 0.8594 - acc: 0.6099 - val_loss: 1.5620 - val_acc: 0.3467\n",
      "Epoch 10/20\n",
      "17776/17776 [==============================] - 323s 18ms/step - loss: 0.7851 - acc: 0.6458 - val_loss: 1.9195 - val_acc: 0.3760\n",
      "Epoch 11/20\n",
      "17776/17776 [==============================] - 328s 18ms/step - loss: 0.7195 - acc: 0.6852 - val_loss: 2.0192 - val_acc: 0.3350\n",
      "Epoch 12/20\n",
      "17776/17776 [==============================] - 322s 18ms/step - loss: 0.6693 - acc: 0.7145 - val_loss: 2.2275 - val_acc: 0.3431\n",
      "Epoch 13/20\n",
      "17776/17776 [==============================] - 329s 19ms/step - loss: 0.6167 - acc: 0.7402 - val_loss: 2.4784 - val_acc: 0.3487\n",
      "Epoch 14/20\n",
      "17776/17776 [==============================] - 328s 18ms/step - loss: 0.5661 - acc: 0.7706 - val_loss: 2.6921 - val_acc: 0.3451\n",
      "Epoch 15/20\n",
      "17776/17776 [==============================] - 329s 19ms/step - loss: 0.5248 - acc: 0.7896 - val_loss: 2.5700 - val_acc: 0.3659\n",
      "Epoch 16/20\n",
      "17776/17776 [==============================] - 327s 18ms/step - loss: 0.4931 - acc: 0.8102 - val_loss: 2.8965 - val_acc: 0.3634\n",
      "Epoch 17/20\n",
      "17776/17776 [==============================] - 328s 18ms/step - loss: 0.4681 - acc: 0.8211 - val_loss: 2.9476 - val_acc: 0.3988\n",
      "Epoch 18/20\n",
      "17776/17776 [==============================] - 326s 18ms/step - loss: 0.4400 - acc: 0.8329 - val_loss: 3.1069 - val_acc: 0.3765\n",
      "Epoch 19/20\n",
      "17776/17776 [==============================] - 327s 18ms/step - loss: 0.4068 - acc: 0.8491 - val_loss: 3.2421 - val_acc: 0.3669\n",
      "Epoch 20/20\n",
      "17776/17776 [==============================] - 329s 19ms/step - loss: 0.3937 - acc: 0.8524 - val_loss: 3.7306 - val_acc: 0.4403\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-39b56f61974c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "train_history = hist\n",
    "loss = train_history.history['acc']\n",
    "val_loss = train_history.history['val_acc']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['acc', 'val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1a9ac285782f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_plot.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/hackaubi/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hackaubi/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hackaubi/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
