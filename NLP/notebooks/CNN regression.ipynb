{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM \n",
    "from keras.utils import plot_model, multi_gpu_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "DATA_PATH = ''\n",
    "EMBEDDING_PATH = './wiki.en.vec'\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "436it [00:00, 4351.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34902it [00:04, 8265.25it/s]Exception KeyError: KeyError(<weakref at 0x7fe650351d60; to 'tqdm' at 0x7fe650350850>,) in <bound method tqdm.__del__ of 34902it [00:04, 8085.82it/s]> ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a12fbbeab988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open(EMBEDDING_PATH, encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('key = ', [u'content', u'language', u'vote_up_count', u'vote_funny_count', u'comment_count', u'steam_weight'])\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "\n",
    "#输入文件路径，返回list;list内包含n个字典形式数据集\n",
    "def read_xlsx(path):\n",
    "    book = xlrd.open_workbook(path)\n",
    "    first_sheet = book.sheet_by_index(0)\n",
    "    result = []\n",
    "    index = 0\n",
    "    #print(first_sheet.row_values(3))\n",
    "    key = first_sheet.row_values(index)\n",
    "    print('key = ',key)\n",
    "    index += 1\n",
    "    while(1):\n",
    "        try:\n",
    "            #print(type(first_sheet.row_values(index)))\n",
    "            dic = dict(zip(key,first_sheet.row_values(index)))\n",
    "            result.append(dic)\n",
    "            index += 1\n",
    "        except Exception:\n",
    "            break\n",
    "    print('done')\n",
    "    # print('result = ',result)\n",
    "    return result, key\n",
    "\n",
    "xlsx_file = './reviews_language_helpful_funny_comment_weight_stripped.xlsx'\n",
    "data_dict, label_name = read_xlsx(xlsx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'comment_count': 0.0,\n",
       "  u'content': u'Todo un clasico ya que no puede faltar en tu coleccion de juegos !!',\n",
       "  u'language': u'spanish',\n",
       "  u'steam_weight': 0.534743,\n",
       "  u'vote_funny_count': 0.0,\n",
       "  u'vote_up_count': 3.0},\n",
       " {u'comment_count': 0.0,\n",
       "  u'content': u'GTA:Hood edition,  With added The Sims\\n\\nBecause every gangsta knows that they need to keep their hair and clothes looking fresh..',\n",
       "  u'language': u'english',\n",
       "  u'steam_weight': 0.47619,\n",
       "  u'vote_funny_count': 0.0,\n",
       "  u'vote_up_count': 0.0},\n",
       " {'class': [0, 0, 1, 0],\n",
       "  u'content': u'IMO, the best GTA. If not just for the voice acting.',\n",
       "  u'language': u'english',\n",
       "  'score': 0.5478677290178756,\n",
       "  u'steam_weight': 0.47619},\n",
       " {'class': [0, 0, 1, 0],\n",
       "  u'content': u'Gimme a ticket',\n",
       "  u'language': u'english',\n",
       "  'score': 0.5478677290178756,\n",
       "  u'steam_weight': 0.489392}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zt(data):\n",
    "\tweight_vote_up = 1.0\n",
    "\tweight_vote_funny = 0.5\n",
    "\tweight_comment = 2.0\n",
    "\tmax = 0.0\n",
    "\ttemp = 0.0\n",
    "\tnew_data = list()\n",
    "\tfor votes in data:\n",
    "\t\tif(votes['vote_up_count']==0.0 and votes['vote_funny_count']==0.0 and votes['comment_count']==0.0):\n",
    "\t\t\tcontinue\n",
    "\t\tvotes['weight_temp']=votes['vote_up_count']*weight_vote_up + votes['vote_funny_count']*weight_vote_funny + votes[ 'comment_count']*weight_comment\n",
    "\t\tnew_data.append(votes)\n",
    "\t\tif(max < votes['weight_temp']):\n",
    "\t\t\tmax = votes['weight_temp']\n",
    "\n",
    "\tzt_list = list()\n",
    "\tzt_gay_list = list()\n",
    "\tfor i in range(int(max*2)+1):\n",
    "\t\tzt_list.append(0)\n",
    "\tfor votes in new_data:\n",
    "\t\tzt_list[int(votes['weight_temp']*2)]=zt_list[int(votes['weight_temp']*2)] + 1\n",
    "\tlenge = len(new_data)\n",
    "\tfor i in range(int(max*2)+1):\n",
    "\t\tzt_gay_list.append((temp+zt_list[i]/2)/lenge)\n",
    "\t\ttemp = temp + zt_list[i]\n",
    "\n",
    "\tfor votes in new_data:\n",
    "\t\tvotes['score'] = zt_gay_list[int(votes['weight_temp']*2)]\n",
    "\t\tif(votes['score']<0.2):\n",
    "\t\t\tvotes['class'] = [1,0,0,0]\n",
    "\t\telif(votes['score']<0.5):\n",
    "\t\t\tvotes['class'] = [0,1,0,0]\n",
    "\t\telif(votes['score']<0.8):\n",
    "\t\t\tvotes['class'] = [0,0,1,0]\n",
    "\t\telse:\n",
    "\t\t\tvotes['class'] = [0,0,0,1]\n",
    "\t\tdel votes['vote_up_count']\n",
    "\t\tdel votes['vote_funny_count']\n",
    "\t\tdel votes['comment_count']\n",
    "\t\tdel votes['weight_temp']\n",
    "\n",
    "\treturn new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'portuguese', u'german', u'japanese', u'spanish', u'polish', u'swedish', u'turkish', u'romanian', u'czech', u'dutch', u'schinese', u'danish', u'bulgarian', u'hungarian', u'ukrainian', u'brazilian', u'french', u'norwegian', u'koreana', u'russian', u'thai', u'tchinese', u'finnish', u'greek', u'english', u'italian'])\n"
     ]
    }
   ],
   "source": [
    "# languages\n",
    "languages = set()\n",
    "for data in data_dict:\n",
    "    languages.add(data['language'])\n",
    "print(languages)\n",
    "current_language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "current_language_data = [data for data in data_dict if data['language'] == current_language ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'comment_count': 0.0,\n",
       "  u'content': u'GTA:Hood edition,  With added The Sims\\n\\nBecause every gangsta knows that they need to keep their hair and clothes looking fresh..',\n",
       "  u'language': u'english',\n",
       "  u'steam_weight': 0.47619,\n",
       "  u'vote_funny_count': 0.0,\n",
       "  u'vote_up_count': 0.0},\n",
       " {u'comment_count': 1.0,\n",
       "  u'content': u'IMO, the best GTA. If not just for the voice acting.',\n",
       "  u'language': u'english',\n",
       "  u'steam_weight': 0.47619,\n",
       "  u'vote_funny_count': 0.0,\n",
       "  u'vote_up_count': 0.0},\n",
       " {u'comment_count': 1.0,\n",
       "  u'content': u'Gimme a ticket',\n",
       "  u'language': u'english',\n",
       "  u'steam_weight': 0.489392,\n",
       "  u'vote_funny_count': 0.0,\n",
       "  u'vote_up_count': 0.0}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_language_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_language_data = zt(current_language_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs_train = [ data['content'] for data in current_language_data]\n",
    "labels = [ data['steam_weight'] for data in current_language_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.517767, 0.494336, 0.516667, 0.409844, 0.51757]\n"
     ]
    }
   ],
   "source": [
    "print(labels[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "#visualize word distribution\n",
    "# train_df['doc_len'] = train_df['comment_text'].apply(lambda words: len(words.split(\" \")))\n",
    "# max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "# sns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc len')\n",
    "# plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n",
    "# plt.title('comment length'); plt.legend()\n",
    "# plt.show()\n",
    "max_seq_len = 0\n",
    "for data in current_language_data:\n",
    "    max_seq_len = max(len(data['content']), max_seq_len)\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1589/24223 [00:00<00:01, 15876.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24223/24223 [00:00<00:00, 26313.50it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"pre-processing train data...\")\n",
    "processed_docs = []\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs.append(\" \".join(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMO best GTA If voice acting',\n",
       " 'Gimme ticket',\n",
       " 'Great PS2 sadly unplayable PC Controller support terrible interface work many resolutions This game aged well 8 10 PS2 0 10 PC Do buy',\n",
       " 'This still best violent sandbox game ever It cool playing observing wicked cool ss hit good game mechanics thinking yep screwed IV Yep forgot IV Mmm hmm include IV For variety occasionally get tiny bit frustrated go something lot IV something tedious Un Fun think hmm element paid lot focus IV drive hitty car go hey kind handling speed IV GTA IV probably worst game ever simply crime coming one completely completely inferior every way shape form Bigger disappointment Duke Nukem Forever']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "('dictionary size: ', 37670)\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs)  #leaky\n",
    "word_seq = tokenizer.texts_to_sequences(processed_docs)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37670"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq = sequence.pad_sequences(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24223, 1455)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUS:8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return len([x.name for x in local_device_protos if x.device_type == 'GPU'])\n",
    "num_gpus = get_available_gpus()\n",
    "print(\"GPUS:{}\".format(num_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 20 \n",
    "num_gpus = num_gpus\n",
    "\n",
    "#model parameters\n",
    "num_filters = 32 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n",
    "learning_rate = 0.001\n",
    "\n",
    "#output parameters\n",
    "num_classes = 4\n",
    "\n",
    "#data split\n",
    "split_persentage = 0.8\n",
    "split_index = int(len(word_seq) * split_persentage)\n",
    "word_seq_train = word_seq[:split_index]\n",
    "word_seq_test = word_seq[split_index:]\n",
    "y_train = y_all[:split_index]\n",
    "y_test = y_all[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 10383\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample words not found:  ['olması' 'netcopy' 'deayy' 'customability' 'วยเหต' 'forceing' 'exacuted'\n",
      " 'antiguity' 'hackusated' '游戏不错' '12th' 'equpiment' 'spectaculat'\n",
      " '1600x900' 'easykills' 'waithing' 'наводки' 'একট' 'physix'\n",
      " 'lcb3j2wc9bquxq']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample words not found: \", np.random.choice(words_not_found, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **BUG FIX**\n",
    "# BUG FIXED: cannot save model while using multi GPU \n",
    "\n",
    "from keras.layers import Lambda, concatenate\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def multi_gpu_model(model, gpus):\n",
    "    if isinstance(gpus, (list, tuple)):\n",
    "        num_gpus = len(gpus)\n",
    "        target_gpu_ids = gpus\n",
    "    else:\n",
    "        num_gpus = gpus\n",
    "        target_gpu_ids = range(num_gpus)\n",
    "\n",
    "    def get_slice(data, i, parts):\n",
    "        shape = tf.shape(data)\n",
    "        batch_size = shape[:1]\n",
    "        input_shape = shape[1:]\n",
    "        step = batch_size // parts\n",
    "        if i == num_gpus - 1:\n",
    "            size = batch_size - step * i\n",
    "        else:\n",
    "            size = step\n",
    "        size = tf.concat([size, input_shape], axis=0)\n",
    "        stride = tf.concat([step, input_shape * 0], axis=0)\n",
    "        start = stride * i\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    all_outputs = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        all_outputs.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU,\n",
    "    # each getting a slice of the inputs.\n",
    "    for i, gpu_id in enumerate(target_gpu_ids):\n",
    "        with tf.device('/gpu:%d' % gpu_id):\n",
    "            with tf.name_scope('replica_%d' % gpu_id):\n",
    "                inputs = []\n",
    "                # Retrieve a slice of the input.\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_i = Lambda(get_slice,\n",
    "                                     output_shape=input_shape,\n",
    "                                     arguments={'i': i,\n",
    "                                                'parts': num_gpus})(x)\n",
    "                    inputs.append(slice_i)\n",
    "\n",
    "                # Apply model on slice\n",
    "                # (creating a model replica on the target device).\n",
    "                outputs = model(inputs)\n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "\n",
    "                # Save the outputs for merging back together later.\n",
    "                for o in range(len(outputs)):\n",
    "                    all_outputs[o].append(outputs[o])\n",
    "\n",
    "    # Merge outputs on CPU.\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for name, outputs in zip(model.output_names, all_outputs):\n",
    "            merged.append(concatenate(outputs,\n",
    "                                      axis=0, name=name))\n",
    "        return Model(model.inputs, merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef9d17c3055c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Model architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training CNN ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m model.add(Embedding(nb_words, embed_dim,\n\u001b[1;32m      5\u001b[0m           weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Model architecture\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim,\n",
    "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "try:\n",
    "    model = ulti_gpu_model(model, gpus=num_gpus)\n",
    "    print(\"Training using multiple GPUs..\")\n",
    "except:\n",
    "    print(\"Training using single GPU or CPU..\")\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "# callbacks_list = [early_stopping]\n",
    "filepath = current_language + '.' + 'weights.ep{epoch:03d}.loss{loss:.3f}.val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17440 samples, validate on 1938 samples\n",
      "Epoch 1/20\n",
      "17440/17440 [==============================] - 12s 708us/step - loss: 0.6997 - acc: 0.0000e+00 - val_loss: 0.6974 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "17440/17440 [==============================] - 6s 339us/step - loss: 0.6971 - acc: 0.0000e+00 - val_loss: 0.6966 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "17440/17440 [==============================] - 6s 330us/step - loss: 0.6963 - acc: 0.0000e+00 - val_loss: 0.6959 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "17440/17440 [==============================] - 6s 327us/step - loss: 0.6957 - acc: 0.0000e+00 - val_loss: 0.6954 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "17440/17440 [==============================] - 6s 333us/step - loss: 0.6953 - acc: 0.0000e+00 - val_loss: 0.6950 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "17440/17440 [==============================] - 6s 326us/step - loss: 0.6949 - acc: 0.0000e+00 - val_loss: 0.6946 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "17440/17440 [==============================] - 6s 333us/step - loss: 0.6945 - acc: 0.0000e+00 - val_loss: 0.6944 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "17440/17440 [==============================] - 6s 326us/step - loss: 0.6942 - acc: 0.0000e+00 - val_loss: 0.6942 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "17440/17440 [==============================] - 6s 330us/step - loss: 0.6939 - acc: 0.0000e+00 - val_loss: 0.6939 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "17440/17440 [==============================] - 6s 324us/step - loss: 0.6936 - acc: 0.0000e+00 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "17440/17440 [==============================] - 6s 330us/step - loss: 0.6932 - acc: 0.0000e+00 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "17440/17440 [==============================] - 6s 326us/step - loss: 0.6929 - acc: 0.0000e+00 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "17440/17440 [==============================] - 6s 327us/step - loss: 0.6925 - acc: 0.0000e+00 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "17440/17440 [==============================] - 6s 325us/step - loss: 0.6922 - acc: 0.0000e+00 - val_loss: 0.6935 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "17440/17440 [==============================] - 6s 326us/step - loss: 0.6920 - acc: 0.0000e+00 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "17440/17440 [==============================] - 6s 326us/step - loss: 0.6917 - acc: 0.0000e+00 - val_loss: 0.6935 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "17440/17440 [==============================] - 6s 325us/step - loss: 0.6914 - acc: 0.0000e+00 - val_loss: 0.6935 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "17440/17440 [==============================] - 6s 325us/step - loss: 0.6913 - acc: 0.0000e+00 - val_loss: 0.6935 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "17440/17440 [==============================] - 6s 325us/step - loss: 0.6911 - acc: 0.0000e+00 - val_loss: 0.6935 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "17440/17440 [==============================] - 6s 330us/step - loss: 0.6910 - acc: 0.0000e+00 - val_loss: 0.6936 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b605912a57ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "train_history = hist\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-1a9ac285782f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_plot.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/hackaubi/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hackaubi/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hackaubi/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
